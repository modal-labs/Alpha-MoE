diff --git a/python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py b/python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py
index b92f215..e3830e0 100644
--- a/python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py
+++ b/python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py
@@ -30,6 +30,12 @@ from .fused_moe_triton_kernels import (
 )
 from .moe_align_block_size import moe_align_block_size
 
+import alpha_moe
+from alpha_moe_python.utils import get_best_config
+from sglang.srt.layers.quantization.fp8_kernel import (
+        sglang_per_token_group_quant_fp8,
+        )
+
 if TYPE_CHECKING:
     from sglang.srt.layers.moe.topk import StandardTopKOutput
 
@@ -405,6 +411,26 @@ def fused_experts_impl(
 
     num_tokens, _ = hidden_states.shape
     E, N, _ = w1.shape
+    M = num_tokens
+    topk = topk_ids.shape[1]
+
+    local_conf = get_best_config(os.getenv("ALPHA_MOE_CONFIG"), M)
+    block_m = local_conf["block_m"]
+    bn = local_conf["block_n"]
+    wn = local_conf["warp_n"]
+    stages = local_conf["stages"]
+    A, A_scale = sglang_per_token_group_quant_fp8(hidden_states, block_shape[1])
+    hidden_states.zero_()
+
+    sorted_token_ids, expert_ids, num_tokens_post_padded = moe_align_block_size(
+            topk_ids, block_m, E
+            )
+    routed_scaling_factor = routed_scaling_factor or 1.0
+
+    torch.ops.alpha_moe.fused_moe_w8a8_up_down(A, A_scale, w1, w1_scale, w2, w2_scale, sorted_token_ids,
+                                                     expert_ids, num_tokens_post_padded, topk_weights, hidden_states,
+                                                     topk, block_m, bn, wn, stages, routed_scaling_factor)
+    return hidden_states
     # We execute the fused_moe kernel in chunks to circumvent this issue:
     # https://github.com/vllm-project/vllm/issues/5938
     CHUNK_SIZE = 64 * 1024
diff --git a/python/sglang/srt/layers/moe/fused_moe_triton/layer.py b/python/sglang/srt/layers/moe/fused_moe_triton/layer.py
index 4924fca..e781648 100644
--- a/python/sglang/srt/layers/moe/fused_moe_triton/layer.py
+++ b/python/sglang/srt/layers/moe/fused_moe_triton/layer.py
@@ -55,6 +55,7 @@ from sglang.srt.utils import (
     is_hip,
     round_up,
 )
+from alpha_moe_python.utils import interleave_tensor
 
 if is_flashinfer_available():
     from flashinfer import fp4_quantize
@@ -252,6 +253,9 @@ class FusedMoE(torch.nn.Module):
         )
 
         self.routing_method_type = routing_method_type
+        self.w13_chunks_loaded = [0] * num_experts
+        self.w13_scale_chunks_loaded = [0] * num_experts
+
 
     def _load_per_tensor_weight_scale(
         self,
@@ -709,6 +713,7 @@ class FusedMoE(torch.nn.Module):
                     expert_data=expert_data,
                     tp_rank=tp_rank,
                 )
+             )
             return
 
         # Case weight scales and zero_points
@@ -742,6 +747,15 @@ class FusedMoE(torch.nn.Module):
                     expert_data=expert_data,
                     tp_rank=tp_rank,
                 )
+                if shard_id in {"w1", "w3"}:
+                    self.w13_scale_chunks_loaded[expert_id] += 1
+                elif shard_id in {"w13"}:
+                    self.w13_scale_chunks_loaded[expert_id] += 2
+
+                if shard_id in {"w1", "w3", "w13"} and self.w13_scale_chunks_loaded[expert_id] == 2:
+                    _interleaved = interleave_tensor(expert_data.view((1,) + expert_data.shape), 1)[0]
+                    expert_data.copy_(_interleaved)
+
             elif quant_method == FusedMoeWeightScaleSupported.TENSOR.value:
                 # INT4-FP8 (INT4 MoE Weight, FP8 Compute): Adjust FP8 per-tensor scaling number for e4m3fnuz (AMD)
                 if _is_hip and get_bool_env_var("SGLANG_INT4_WEIGHT"):
@@ -776,6 +790,15 @@ class FusedMoE(torch.nn.Module):
                 expert_data=expert_data,
                 tp_rank=tp_rank,
             )
+            if shard_id in {"w1", "w3"}:
+                self.w13_chunks_loaded[expert_id] += 1
+            elif shard_id in {"w13"}:
+                self.w13_chunks_loaded[expert_id] += 2
+
+            if shard_id in {"w1", "w3", "w13"} and self.w13_chunks_loaded[expert_id] == 2:
+                _interleaved = interleave_tensor(expert_data.view((1,) + expert_data.shape))[0]
+                expert_data.copy_(_interleaved)
+
             return
 
     def weight_loader_fused(
